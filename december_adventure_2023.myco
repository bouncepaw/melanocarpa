**December adventure** is a yearly challenge. The goal is to code //something// every day. Anything. I'm in this year.

=> https://eli.li/december-adventure

**NB.** As of the start December 2023, I'm still not done with [[Inktober 2023]].

**December 1st.** Doing a Java assignment for the University. Somehow installed an outdated Mongo DB. I tried to install a new one, but the modern tech hates the old 2020 OS I use on [[Caemlyn]]. The outdated one was found in Nix. Thanks Nix, the only package manager that never fails to install. I didn't learn Mongo though, not yet.

Then I started integrating Lombok, which is a library to reduce boilerplate. Pretty nice!

And during the night I was starting to get the 100 % code coverage. There's still job to be done!

**December 2nd.** Continuing doing the same assignment. Writing more tests, increasing the coverage. One of the subtasks is to implement the conversion from our classes to BSON documents. Made a //Factory//. I feel like a serious enterprise Java developer. Have I told you that I want to implement [[Betulopticon]] in Java?

Thought about [[Uxn Legoptics]]. Surely, if I ever do it, there will be no Java in it. Wrong tool.

Wrote more tests. Ran a //SpotBugs// lexical analysis. I don't wanna fix this...

**December 3rd.** Postponing Java, doing the Information theory task I have. We've got a table with information about Titanic passengers, one of the column says if they were dead or alive. We gotta make a simple model for predicting if we would have lived. The instructions are detailed enough, but I am getting weird results for some reason...

I am doing this task in R btw.

**December 4th.** Continuing the R task. I am the only one doing it in R, by the way. Most people are doing it in Python+NumPy or C#. NumPy is a good choice, of course. But why would they go with C#? I asked them. They said they are just used to it. Get used to something more fitting the task! Oh, whatever.

So, my problem yesterday was that the data was not normalized. After Z-normalizing it, everything worked. For your information, //my// chances of surviving on the Titanic shipwreck would have been 44 %. That's what they numbers say!

**December 5th.** Turned in the R assignment. I also improved it during the classes. The teacher marked down a plus, as he always does. One plus remaining. Look at the code sample!:

```r
# ...
zdata <- (t(as.matrix(data)) - data.mean) / data.stddev
zdata[1, ] = 1

log.mle <- function(theta) sum(log(
	((1/(
		1 + exp(-crossprod(theta, zdata))
	)) ^ outcomes) * ((1/(
		1 + exp(crossprod(theta, zdata))
	)) ^ (1 - outcomes))
))

dot <- function(a, b) a %*% b
mygrad <- function(theta) {
	res <- 0
	for (i in 1:n) {
		x.i <- as.numeric(zdata[, i])
		a <- as.numeric(outcomes[i] - 1/(1 + exp(-dot(theta, x.i))))
		res = res + a * x.i
	}
	res
}
# ...
```

I think the `mygrad` function (that is, my gradient function) could have been made more fancy with some matrix multiplication, but I didn't have enough thoughts for that back then, and now the task is turned in and I don't have time for any extra.

Finally understood what am I to do for the Computer Networks assignment! Understanding was the tricky part. OK I now have the knowledge!

Right before midnight started doing the Computer Graphics task. Will continue tomorrow. The task is massive. Will have to devote a lot of time to it. Today I programmed the various affine transformation matrices. For example:

```python
def mx_rotate_oz(angle):
	angle *= radians_in_degree
	return np.array([
		[cos(angle), -sin(angle), 0, 0],
		[sin(angle),  cos(angle), 0, 0],
		[         0,           0, 1, 0],
		[         0,           0, 0, 1]
	])
```
